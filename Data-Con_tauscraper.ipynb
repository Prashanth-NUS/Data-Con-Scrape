{
 "metadata": {
  "name": "",
  "signature": "sha256:e4b8e20a7ed5f17a8fef09121cd88c1512df669a02f62f5977d53699b1679eeb"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##DataTau scraper\n",
      "\n",
      "This ended up to be a fully functional scraper for the popular data links aggregator, datatau.com\n",
      "\n",
      "###First, our usual checks. \n",
      "API? Noooo. \n",
      "\n",
      "Terms of service? Not so much. So, we're cleared for scrape-off.   \n",
      "(lollolololol. that is really bad.)\n",
      "\n",
      "###So let's walk through the scraper."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "sys.path.append('./datatau_scraper')\n",
      "\n",
      "import tauscraper"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###You can check out all of this code in datatau_scraper/tauscraper.py\n",
      "\n",
      "We make a `TauCollection` object, which is the class that does most of the stuff. tauscraper also has an `TauItem` class, which we'll get to later."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "collection = tauscraper.TauCollection()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###After instantiating the collection,\n",
      "\n",
      "You can run the entire scraper using `collection.scrape_datatau()`\n",
      "You end up with a collection object full of tasty data (which you can explore below if you're running this notebook interactively), and it also archives two .pkl files (one with a dictionary of data, and one that collects the raw html). \n",
      "\n",
      "####Oops, TODO: \n",
      "Just realized that the point of the raw html collection was in case the scrapers fail, the old data will still be available (like I recommended to someone in the Q&A). However, this will not do that, because I forgot to. I'm very sure that everything (dictionary and raw storage) will succeed, or it will all just fail. Let's call this \"to-be-implemented\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "collection = collection.scrape_datatau()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "could not load tau item dict pickle. creating new dict\n",
        "could not load raw pages pickle. creating new archive\n",
        "filling the queue\n",
        "next url:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.datatau.com/x?fnid=6BnXSS8znj\n",
        "next url:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.datatau.com/x?fnid=w8L9Akuz6k\n",
        "next url:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.datatau.com/x?fnid=Ua2xkrGkxU\n",
        "next url:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.datatau.com/x?fnid=t8FKRPR9pp\n",
        "next url:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.datatau.com/x?fnid=mZlD4qXEvh\n",
        "next url:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.datatau.com/x?fnid=4VrtnYRZiO\n",
        "reached end of items"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "soup queue full, queued 7 pages\n",
        "processing page 1\n",
        "soup to rows\n",
        "rows to items\n",
        "processing items\n",
        "processing page 2\n",
        "soup to rows\n",
        "rows to items\n",
        "processing items\n",
        "processing page 3\n",
        "soup to rows\n",
        "rows to items\n",
        "processing items\n",
        "processing page"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4\n",
        "soup to rows\n",
        "rows to items\n",
        "processing items\n",
        "processing page 5\n",
        "soup to rows\n",
        "rows to items\n",
        "processing items\n",
        "processing page 6\n",
        "soup to rows\n",
        "rows to items\n",
        "processing items\n",
        "processing page 7\n",
        "soup to rows\n",
        "rows to items\n",
        "processing items\n",
        "pickling the data"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "ename": "IOError",
       "evalue": "[Errno 2] No such file or directory: 'data/tau_raw_pages.pkl'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-4-4a966d492e51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcollection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscrape_datatau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m/Users/skelly/laurie_projects/Data-Con-Scrape/datatau_scraper/tauscraper.py\u001b[0m in \u001b[0;36mscrape_datatau\u001b[0;34m(self, append)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpickle_me\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/skelly/laurie_projects/Data-Con-Scrape/datatau_scraper/tauscraper.py\u001b[0m in \u001b[0;36mpickle_me\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpickle_me\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m'pickling the data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRAW_PAGE_PICKLEFILE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutfile1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_pages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutfile1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTAU_DICT_PICKLEFILE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutfile2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'data/tau_raw_pages.pkl'"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "collection = collection.load_data_from_pickles()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "loaded tau item dict from pickle\n",
        "loaded raw pages dict from pickle\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#collection.tau_dict\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "7"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "\n",
      "with open('tau_item_dict.pkl','rb') as infile:\n",
      "    tau_data = pickle.load(infile)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(tau_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "210"
       ]
      }
     ],
     "prompt_number": 5
    }
   ],
   "metadata": {}
  }
 ]
}