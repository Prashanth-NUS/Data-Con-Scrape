{
 "metadata": {
  "name": "",
  "signature": "sha256:975f979ce7f878ab6620d6819c224a13a56f3fc42ebdeeb884c156bc250ef8af"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##What we'll be using in this workshop\n",
      "* Python 2.7\n",
      "\n",
      "###Things you might have to install if you want to work along\n",
      "* [ipython notebook](http://ipython.org/ipython-doc/dev/notebook/)\n",
      "  - `pip install ipython[notebook]`\n",
      "* [requests](http://docs.python-requests.org/en/latest/)\n",
      "  - `pip install requests`\n",
      "* [BeautifulSoup](http://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
      "  - `pip install bs4`\n",
      "* [PIL (python image library)](http://effbot.org/imagingbook/pil-index.htm#module-reference)\n",
      "  - `pip install pillow`\n",
      "* [selenium](http://selenium-python.readthedocs.org/)\n",
      "  - *(see module below for installation instructions)*\n",
      "* [dateutil](https://labix.org/python-dateutil#head-a23e8ae0a661d77b89dfb3476f85b26f0b30349c)\n",
      "  - `pip install python-dateutil`\n",
      "\n",
      "\n",
      "###Packages we'll use from the standard library \n",
      "* [pickle](https://docs.python.org/2/library/pickle.html)\n",
      "* [StringIO](https://docs.python.org/2/library/stringio.html)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Selenium installation\n",
      "\n",
      "* for scraping with selenium webdriver, we will need to install it first:  \n",
      "```\n",
      "pip install selenium\n",
      "```\n",
      "\n",
      "* If you don't have Google Chrome, install it: https://www.google.com/chrome/browser/  \n",
      "\n",
      "\n",
      "* download the selenium \"driver\" for chrome, from [here](https://code.google.com/p/selenium/wiki/ChromeDriver)\n",
      "\n",
      "  - The chrome driver is a single file: Put it somewhere accessible (your home directory or the directory where you are running this file are good options). \n",
      "\n",
      "  - When you're initiating the driver in python, you will specify the path to this chromedriver file you downloaded.\n",
      "\n",
      "```\n",
      "from selenium import webdriver\n",
      "driver = webdriver.Chrome('./chromedriver')\n",
      "```\n",
      "(Change the path `'./chromedriver'` to the directory where you put your chromedriver file)\n",
      "\n",
      "*Note*: In case your computer complains about the driver even though it really is in the path you specified, I've found this [stackoverflow answer](http://stackoverflow.com/questions/8255929/running-webdriver-chrome-with-selenium) helpful for troubleshooting. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Scrape Project 1:\n",
      "###data-con face sheet"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##tools: python Requests\n",
      "The `requests` package in python is awesome.  \n",
      "Check it out if you are used to using `urllib2`\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests\n",
      "\n",
      "# requests creates a request object and makes the request\n",
      "r = requests.get('http://data-con.org/schedule/')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our variable `r` now holds a response object"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'r is an instance of',type(r)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we want to see the request we sent to get the response, it is kept as an attribute of the response object `r`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# checking out the r.request object\n",
      "print 'r.request = %s'%repr(r.request)\n",
      "print '\\nURL of the request:'\n",
      "print 'r.request.url = %s'%r.request.url\n",
      "print '\\n Headers:'\n",
      "\n",
      "from pprint import pprint\n",
      "pprint(dict(r.request.headers))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "r is an instance of <class 'requests.models.Response'>\n",
        "r.request = <PreparedRequest [GET]>\n",
        "\n",
        "URL of the request:\n",
        "r.request.url = http://data-con.org/schedule/\n",
        "\n",
        " Headers:\n",
        "{'Accept': '*/*',\n",
        " 'Accept-Encoding': 'gzip, deflate, compress',\n",
        " 'User-Agent': 'python-requests/1.2.3 CPython/2.7.2 Darwin/12.5.0'}\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### back to scraping \n",
      "The response object r holds the html of the page we want to scrape in r.content"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#print r.content\n",
      "# if you are in the published notebook, imagine an output window full of visual noise recognizable as html. \n",
      "# in interactive mode, uncomment the top line and run the cell to witness for yourself. "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To parse this content and extract the information I want, I use Beautiful Soup\n",
      "\n",
      "##BeautifulSoup"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from bs4 import BeautifulSoup\n",
      "\n",
      "soup = BeautifulSoup(r.content)\n",
      "#print soup\n",
      "# at first glance, soup and r.content look the same, but they are not."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's parse the page for all the links and then see how we can narrow down to what we want."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# These next few cells show the iterative process of narrowing down your selectors to get the desired output.\n",
      "# that's why they look super redundant.\n",
      "links = soup.find_all('a')\n",
      "for a in links: \n",
      "    print '%s: %s'%(a.text.strip(),a['href'])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "links = soup.find_all('a')\n",
      "print len(links)\n",
      "links = [l for l in links if 'data-con.org' in l['href']]\n",
      "print len(links)\n",
      "\n",
      "for a in links: \n",
      "    print '%s: %s'%(a.text.strip(),a['href'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "49\n",
        "46\n",
        ": http://data-con.org/\n",
        "Boston Data-Con 2014: http://data-con.org/\n",
        "Register: http://data-con.org/register/\n",
        "Schedule: http://data-con.org/schedule/\n",
        "Directions: http://data-con.org/directions/\n",
        "Floorplan: http://data-con.org/microsoft-nerd/\n",
        "Contact: http://data-con.org/contact/\n",
        "R Beginner Bootcamp - Joe Kambourakis and John Verostek: http://data-con.org/r-bootcamp/\n",
        "Introduction to the Data Science Method  - David Weisman: http://data-con.org/david-weisman/\n",
        "Using Twitter to Analyze Switching Across Cellphone Carriers  - Tanya Cashorali: http://data-con.org/tanya-cashorali/\n",
        "Topic Modelling Using R - Herb Susmann: http://data-con.org/herb-susmann/\n",
        "Combining R Libraries into Automated Workflows - Dag Holmboe: http://data-con.org/dag-holmboe/\n",
        "General Linearized Mixed Models (GLMMs) in R  - Julia Pilowsky: http://data-con.org/julia-pilowsky/\n",
        "Optimizing Multilingual Search Using Solr- David Troiano: http://data-con.org/david-troiano/\n",
        "Mining of Massive Datasets Using Locality Sensitive Hashing (LSH) - J Singh and Teresa Brooks: http://data-con.org/j-singh-and-teresa-brooks/\n",
        "DataViz Design Principles  - Angela Bassa: http://data-con.org/angela-bassa/\n",
        "Massive Feature Selection Using Supercomputing in R - Jean-Loup Loyer: http://data-con.org/jean-loup-loyer/\n",
        "Python DataViz Tour - Ian Stokes-Rees: http://data-con.org/ian-stokes-rees/\n",
        "Robots, Small Molecules & R - Ingredients for Exploring and Predicting Biological Effects - Rajarshi Guha: http://data-con.org/rajarshi-guha/ \n",
        "Interactive DataViz with R:  ggvis, rCharts, Shiny - Abhinav Sarapure: http://data-con.org/abhinav-sarapure/ \n",
        "High Dimensionality in Large Datasets - Sri Krishnamurthy:  http://data-con.org/sri-krishnamurthy/\n",
        "A Case Study Visualizing Boston's Subway System Using D3 and other Open Source tools  - Mike Barry and Brian Card: http://data-con.org/mike-barry-and-brian-card/\n",
        "Principles of Data Engineering- Edmund Jorgenson and Matt Papi: http://data-con.org/edmund-jorgensen-and-matt-papi/\n",
        "Baseball and Data Engineering using Statistics, R & Python - Dan Milstein:  http://data-con.org/dan-milstein/\n",
        "iPython Tutorial - Imran Malek: http://data-con.org/imran-malek/\n",
        "Creating Custom Big Data Tools including Models, Hadoop Clusters, and DataViz - DigitasLBI: http://data-con.org/digitaslbi/\n",
        "Introduction to Massively Parallel Databases  - Wes Reing: http://data-con.org/wes-reing/\n",
        "Regression Analysis with Python, Pandas, and StatsModels - Allen Downey: http://data-con.org/allen-downey/\n",
        "Scaling R with ScaleR Packages - Steve Belcher: http://data-con.org/steve-belcher/\n",
        "More Pandas! - Mali Akmanalp: http://data-con.org/mali-akmanalp/\n",
        "Python Data Mining Using Orange Canvas - Justin Sun: http://data-con.org/justin-sun/\n",
        "Open-Source Data-Analysis for Bio-tech - Will Sutton: http://data-con.org/will-sutton/\n",
        "Introduction to Hive with Case Study on Storing and Querying Protobuf Logs in Hive - Muralikumar Venkat: http://data-con.org/muralikumar-venkat/\n",
        "Gamification and Big Data - Nick Lim: http://data-con.org/nick-lim/\n",
        "Lunchtime Talk:  Visualizations for Exploring  Data - Pratik Lundblad: http://data-con.org/patrik-lundblad/ \n",
        "Glue: a hackable user interface for multidimensional data exploration  - Chris Beaumont: http://data-con.org/chris-beaumont/\n",
        "Data Science, YouTube, & Media Disruption - Pete Martin of Pixability: http://data-con.org/pete-martin/\n",
        "Building Predictive Models in Cloud using Microsoft Azure Machine Learning  - Roope Astala: http://data-con.org/roope-astala/\n",
        "Statistical inference in Python the NIFTY way  - Mike Bell: http://data-con.org/mike-bell/\n",
        "DrivenData.org:   Python-based Site for Data Science Competitions - Greg Lipstein & Peter Bull: http://data-con.org/greg-lipstein-and-peter-bull/\n",
        "Using Python's Machine Learning and Dynamic Control Libraries for Online Advertisement Analysis - Michael Els: http://data-con.org/michael-els/\n",
        "R Beginner Bootcamp - Joe Kambourakis and John Verostek: http://data-con.org/r-bootcamp/\n",
        "IP-Reputation Scoring System in Python and Hadoop  - Stuart Layton: http://data-con.org/stuart-layton/\n",
        "Web Scraping Using Python's Beautiful Soup and Selenium - Laurie Skelly: http://data-con.org/laurie-skelly/\n",
        ": http://data-con.org/wp-content/uploads/2014/09/sponsors-0907.png\n",
        "Boston Data-Con 2014: http://data-con.org/\n"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "links = soup.find_all('a')\n",
      "print len(links)\n",
      "links = [l for l in links if 'data-con.org' in l['href']]\n",
      "print len(links)\n",
      "links = [l for l in links if ' - ' in l.text]\n",
      "print len(links)\n",
      "\n",
      "# for a in links: \n",
      "#     print '%s: %s'%(a.text.strip(),a['href'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "49\n",
        "46\n",
        "35\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have narrowed the links down to the ones of interest, we'll extract the urls from the BeautifulSoup anchor tag obects. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "hrefs = [a['href'].strip() for a in links]\n",
      "#pprint(hrefs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We'll want to go to each of these pages and extract information. \n",
      "\n",
      "We can start writing functions to make this process more streamlined and reuseable."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def connection(url):\n",
      "    r = requests.get(url)\n",
      "    soup = None\n",
      "    if r.ok:\n",
      "        soup = BeautifulSoup(r.text)\n",
      "    else:\n",
      "        print 'request unsuccessful with code %s'%response.status_code\n",
      "    return soup"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I will loop through each of my session links in `href` and use my new function `connection(url)` to get the raw content, storing it in a list called `raw_sessions`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "raw_sessions = []\n",
      "\n",
      "for url in hrefs:\n",
      "    print 'getting/souping: %s'%url\n",
      "    soup = connection(url)\n",
      "    raw_sessions.append(soup.find(id='content'))\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import pickle\n",
      "# always good to make yourself some insurance\n",
      "# REMEMBER: you can't pickle a soup so you have to convert the soup object to strings using prettify() first.\n",
      "raw_sessions = [s.prettify() for s in raw_sessions]\n",
      "\n",
      "# open a file and use pickle to store the list of strings (html)\n",
      "with open('insurance/datacon_contents_raw.pkl','w') as outfile:\n",
      "     pickle.dump(raw_sessions,outfile)\n",
      "\n",
      "!ls -lah ./insurance/\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "total 1112\r\n",
        "drwxr-xr-x   5 skelly  staff   170B Sep 14 12:16 \u001b[1m\u001b[34m.\u001b[m\u001b[m\r\n",
        "drwxr-xr-x  15 skelly  staff   510B Sep 14 11:47 \u001b[1m\u001b[34m..\u001b[m\u001b[m\r\n",
        "-rw-r--r--   1 skelly  staff    25K Sep 14 11:47 data_con_html.txt\r\n",
        "-rw-r--r--   1 skelly  staff    97K Sep 14 12:16 datacon_contents_raw.pkl\r\n",
        "-rw-r--r--   1 skelly  staff   428K Sep 14 12:09 datacon_soups_raw.pkl\r\n"
       ]
      }
     ],
     "prompt_number": 77
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#insurance sanity check\n",
      "import pickle\n",
      "with open('insurance/datacon_contents_raw.pkl','r') as infile:\n",
      "    raw_sessions = [BeautifulSoup(s) for s in pickle.load(infile)]\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# working through getting all the content I want from the page before I automate\n",
      "\n",
      "session = BeautifulSoup(raw_sessions[-1])\n",
      "# print session\n",
      "\n",
      "table = session.find('table')\n",
      "# print table\n",
      "\n",
      "speaker = table.find('center').text.strip()\n",
      "print speaker\n",
      "\n",
      "picture = table.find('img')\n",
      "print picture\n",
      "picture_src = picture['src']\n",
      "print picture_src"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's make some more functions to parse the page information"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_datacon_facesheet(faces,nrows=7,ncols=5):\n",
      "    #faces = iter(get_faces_from_sessions(raw_sessions))\n",
      "    faces = iter(faces)\n",
      "    with open('facesheet.md','w') as outfile:\n",
      "        outfile.write('| . '*ncols + '|\\n')\n",
      "        outfile.write('|----'*ncols + '|\\n')\n",
      "        for row in range(nrows):\n",
      "            row = '|'\n",
      "            for col in range(ncols):\n",
      "                row += '%s  ![](%s) |'%faces.next()\n",
      "            outfile.write('%s|\\n'%row)\n",
      "\n",
      "    return \n",
      "    \n",
      "def get_faces_from_sessions(raw_sessions):\n",
      "    faces = []\n",
      "    for session in raw_sessions:\n",
      "        table = session.find('table')\n",
      "        rows = table.find_all('tr')\n",
      "        num_speakers = len(rows[1].find_all('img'))\n",
      "        print 'speakers in this session: %i'%num_speakers\n",
      "        \n",
      "        for i in range(num_speakers):\n",
      "            top_row = rows[0]\n",
      "#            print top_row\n",
      "            speaker = top_row.find_all('th')[i].text.strip()\n",
      "            speaker = ' '.join(speaker.split())\n",
      "            picture_src = rows[1].find_all('td')[i].find('img')['src']\n",
      "            local_picture_src = save_speaker_picture(speaker,picture_src)\n",
      "        \n",
      "            faces.append((speaker,local_picture_src))\n",
      "    return faces"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r = requests.get(picture_src)\n",
      "# this will give us a bunch of binary nonsense, (which you can check if you care to)\n",
      "# print r.contents\n",
      "# we can write it to an image file using the package PIL\n",
      "\n",
      "from PIL import Image\n",
      "from StringIO import StringIO\n",
      "\n",
      "def save_speaker_picture(speaker,picture_src):\n",
      "    print 'retrieving picture for %s'%speaker\n",
      "    r = requests.get(picture_src)\n",
      "    i = Image.open(StringIO(r.content))\n",
      "    filename = '_'.join(speaker.lower().split()) + '.jpg'\n",
      "    filename = './img/'+filename\n",
      "\n",
      "    with open(filename,'w') as outfile:\n",
      "        i.save(filename,'JPEG')\n",
      "\n",
      "    return filename"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "All of these functions are saved as datacon_scrape.py"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "faces = get_faces_from_sessions(raw_sessions)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "speakers in this session: 2\n",
        "retrieving picture for Joe Kambourakis\n",
        "retrieving picture for John Verostek"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "speakers in this session: 1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "retrieving picture for David Weisman\n",
        "speakers in this session: 1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "retrieving picture for Tanya Cashorali\n",
        "speakers in this session: 1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "retrieving picture for Herb Susmann\n",
        "speakers in this session: 1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "retrieving picture for Speaker\n",
        "speakers in this session: 0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "speakers in this session: 2\n",
        "retrieving picture for J Singh\n",
        "retrieving picture for Teresa Brooks"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "speakers in this session: 1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "retrieving picture for Angela Bassa\n",
        "speakers in this session: 1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "retrieving picture for Jean-Loup Loyer\n",
        "speakers in this session: 1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "retrieving picture for Ian Stokes-Rees\n",
        "speakers in this session: 1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "retrieving picture for Rajarshi Guha\n",
        "speakers in this session: 1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "retrieving picture for Abhinav Sarapure\n",
        "speakers in this session: 1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "retrieving picture for Sri Krishnamurthy\n",
        "speakers in this session: 2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "retrieving picture for Mike Barry\n",
        "retrieving picture for Brian Card"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "speakers in this session: 1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "retrieving picture for Dan Milstein\n",
        "speakers in this session: 1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "retrieving picture for Imran Malek\n",
        "speakers in this session: 1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "retrieving picture for DigitasLBI\n",
        "speakers in this session: 1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "retrieving picture for Wes Reing\n",
        "speakers in this session: 1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "retrieving picture for Allen Downey\n",
        "speakers in this session: 1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "retrieving picture for Steve Belcher\n",
        "speakers in this session: 1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "retrieving picture for Speaker\n",
        "speakers in this session: 0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "speakers in this session: 1\n",
        "retrieving picture for Will Sutton\n",
        "speakers in this session: 0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "speakers in this session: 1\n",
        "retrieving picture for Nick Lim\n",
        "speakers in this session: 1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "retrieving picture for Patrik Lundblad\n",
        "speakers in this session: 1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "retrieving picture for Chris Beaumont\n",
        "speakers in this session: 1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "retrieving picture for Pete Martin\n",
        "speakers in this session: 1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "retrieving picture for Roope Astala\n",
        "speakers in this session: 1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "retrieving picture for Mike Bell\n",
        "speakers in this session: 0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "speakers in this session: 1\n",
        "retrieving picture for Speaker\n",
        "speakers in this session: 2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "retrieving picture for Joe Kambourakis\n",
        "retrieving picture for John Verostek"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "speakers in this session: 1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "retrieving picture for Speaker\n",
        "speakers in this session: 1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "retrieving picture for Laurie Skelly\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "make_datacon_facesheet(faces)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### as mentioned, here's the snippet to disguise your python scraper as a browser by altering the request headers\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "# sometimes things work more smoothly if we change the User-Agent in the request headers\n",
      "def connection(url):\n",
      "    headers = {'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_5)'}\n",
      "    r = requests.get(url,headers=headers)\n",
      "    soup = None\n",
      "    if r.ok:\n",
      "        soup = BeautifulSoup(r.text,'html5lib')\n",
      "    else:\n",
      "        print 'request unsuccessful with code %s'%response.status_code\n",
      "    return soup"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    }
   ],
   "metadata": {}
  }
 ]
}